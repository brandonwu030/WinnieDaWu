{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e1f39c-4d67-44e2-aea1-77ce872a8c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n[[45976 18136]\n [ 3204  6556]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.72      0.81     64112\n           1       0.27      0.67      0.38      9760\n\n    accuracy                           0.71     73872\n   macro avg       0.60      0.69      0.60     73872\nweighted avg       0.85      0.71      0.75     73872\n\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from Spark table\n",
    "df_spark = spark.table(\"default.lol_raw_data_updated\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "# Convert dates\n",
    "df['OBS DATE'] = pd.to_datetime(df['OBS DATE'])\n",
    "df['LASTLOGIN'] = pd.to_datetime(df['LASTLOGIN'])\n",
    "\n",
    "# Churn label: 1 if inactive 31+ days\n",
    "df['CHURN'] = ((df['OBS DATE'] - df['LASTLOGIN']).dt.days >= 31).astype(int)\n",
    "\n",
    "# Clean tier data\n",
    "df['TIER_CLEANED'] = df['TIER'].astype(str).str.upper().str.extract(r'([A-Z]+)')\n",
    "tier_order = {\n",
    "    'GOLD': 3, 'PLATINUM': 4, 'EMERALD': 5,\n",
    "    'DIAMOND': 6, 'MASTER': 7, 'GRANDMASTER': 8, 'CHALLENGER': 9\n",
    "}\n",
    "df['TIER_ENCODED'] = df['TIER_CLEANED'].map(tier_order)\n",
    "\n",
    "# Clean 'WIN PER' and 'LAST 20 WIN PER' columns by removing 'L' and '%', and keeping only numeric\n",
    "for colname in ['WIN PER', 'LAST 20 WIN PER']:\n",
    "    df[colname] = df[colname].astype(str).str.replace('L', '', regex=False).str.replace('%', '', regex=False)\n",
    "    df = df[df[colname].str.replace('.', '', regex=False).str.isnumeric()]\n",
    "    df[colname] = df[colname].astype(float)\n",
    "\n",
    "# Clean 'WIN PER' columns\n",
    "for colname in ['WIN PER', 'LAST 20 WIN PER']:\n",
    "    df[colname] = df[colname].astype(str).str.replace('%', '', regex=False).astype(float)\n",
    "\n",
    "\n",
    "# Define features\n",
    "features = [\n",
    "    'LP', 'LEVEL', 'AVSCORE', 'WIN PER', 'LAST 20 WIN PER', 'TOTAL MATCH', 'LOSING STREAK',\n",
    "    'NO CHAMPIONS PLAYED', 'TIER_ENCODED', 'NO TEAM GAMES',\n",
    "    'NO TEAM PARTICIPANTS', 'TEAM WIN', 'TEAM LOSE'\n",
    "]\n",
    "\n",
    "# Drop missing data\n",
    "df_model = df[features + ['CHURN']].dropna()\n",
    "\n",
    "# Split into X and y\n",
    "X = df_model[features]\n",
    "y = df_model['CHURN']\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f40e86-95a7-4132-86b6-b6bb6fd01ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/04 18:51:08 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2025/12/04 18:51:45 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411dec9c-c6d7-4561-8f0e-6fef2ed4c7ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save your trained model\n",
    "joblib.dump(rf_model, \"rf_churn_model.pkl\", compress=3)\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Model saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "random_forest_lol_churn _prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}